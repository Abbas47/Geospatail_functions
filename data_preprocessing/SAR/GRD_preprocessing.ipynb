{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import libraries\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/.snap/snap-python')\n",
    "import time\n",
    "start = time.time()\n",
    "import os, gc\n",
    "import boto3\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from shapely.geometry import Polygon\n",
    "from snappy import GPF, ProductIO, HashMap, jpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_exist(path):\n",
    "    '''\n",
    "    Function for checking the exisiting path and if the path doesnt exist the give path will be created.\n",
    "    Input parameters:\n",
    "    1. path: file path\n",
    "    '''\n",
    "    # check whether the specified path exists or not\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        # create a new directory\n",
    "        os. makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### User input\n",
    "## input for path\n",
    "# main-foldername/bucket-name\n",
    "main_foldername = 'data'\n",
    "\n",
    "# sub-main folders that comes under main folder \n",
    "submain_folder = 'SAR'\n",
    "\n",
    "# folders that comes under weekly folder\n",
    "raw_imagery = 'raw_img'\n",
    "preprocessed_imagery = 'preproc_img'\n",
    "band_folder_vh = 'VH'\n",
    "band_folder_vv = 'VV'\n",
    "\n",
    "## input for functions\n",
    "proj = 'WGS84(DD)' # define projection\n",
    "downsample = 0 # input 0 or 1 (1 -- need downsample to 40m and 0 -- no need to downsample)\n",
    "subset = False # input, i.e., True or False. if download=True, the terrain corrected data will be cliped based on the polygon provided by user and if download=False, the terrain corrected data will not be cliped\n",
    "poly_wkt = '' # define the polygon in wkt format for cliping the terrain corrected data\n",
    "''' well-known-text (WKT) file for subsetting (can be obtained from SNAP by drawing a polygon)\n",
    "    wkt = 'POLYGON ((-157.79579162597656 71.36872100830078, -155.4447021484375 71.36872100830078, \\\n",
    "    -155.4447021484375 70.60020446777344, -157.79579162597656 70.60020446777344, -157.79579162597656 71.36872100830078))' '''\n",
    "download = False # input, i.e., True or False. if download=True, the relevent data required for executing this script will be downloaded from s3 to local instance and if download=False, then the data will not be downloaded from s3. Deafult to False\n",
    "remove = False # input, i.e., True or False. if remove=True, the data used and generated from this script will be removed from local instance and if remove=False, then the data will not be removed from local instance. Deafult to False\n",
    "\n",
    "\n",
    "### Define path\n",
    "base_path = main_foldername\n",
    "SAR_folder_path = os.path.join(base_path, submain_folder)\n",
    "raw_imagery_path = os.path.join(SAR_folder_path, raw_imagery)\n",
    "preprocessed_imagery_path = os.path.join(SAR_folder_path, preprocessed_imagery)\n",
    "preprocessed_imagery_path_vh = os.path.join(preprocessed_imagery_path, band_folder_vh)\n",
    "preprocessed_imagery_path_vv = os.path.join(preprocessed_imagery_path, band_folder_vv)\n",
    "\n",
    "# path_exist(preprocessed_imagery_path)\n",
    "path_exist(preprocessed_imagery_path_vh)\n",
    "path_exist(preprocessed_imagery_path_vv)\n",
    "\n",
    "# Declare bucket name, remote file, and destination for S3 and local\n",
    "my_bucket = main_foldername\n",
    "bucket_prefix = f'{submain_folder}/{raw_imagery}' # path of the directory where the raw imageries seats in the s3 bucket\n",
    "\n",
    "# location for uploading the data from ec2 to s3\n",
    "drive_path_vh = f'{submain_folder}/{preprocessed_imagery}/{band_folder_vh}'\n",
    "drive_path_vv = f'{submain_folder}/{preprocessed_imagery}/{band_folder_vv}'\n",
    "local_path = preprocessed_imagery_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function\n",
    "def do_apply_orbit_file(source):\n",
    "    '''\n",
    "    Function to do orbit correction\n",
    "    Input parameters:\n",
    "    1. source: Opened SAR imagery using snappy.ProductIO.readProduct(file_path)\n",
    "    Output:\n",
    "    Return orbit corrected data\n",
    "    '''\n",
    "    print('\\tApply orbit file...')\n",
    "    parameters = HashMap()\n",
    "    parameters.put('orbitType', 'Sentinel Precise (Auto Download)')\n",
    "    parameters.put('polyDegree', '3')\n",
    "    # parameters.put('Apply-Orbit-File', True)\n",
    "    output = GPF.createProduct('Apply-Orbit-File', parameters, source)\n",
    "    return output\n",
    "\n",
    "def do_thermal_noise_removal(source):\n",
    "    '''\n",
    "    Function to do thermal correction\n",
    "    Input parameters:\n",
    "    1. source: Orbit corrected data\n",
    "    Output:\n",
    "    Return thermal corrected data\n",
    "    '''\n",
    "    print('\\tThermal noise removal...')\n",
    "    parameters = HashMap()\n",
    "    parameters.put('removeThermalNoise', True)\n",
    "    output = GPF.createProduct('ThermalNoiseRemoval', parameters, source)\n",
    "    return output\n",
    "\n",
    "def do_calibration(source, polarization, band):\n",
    "    '''\n",
    "    Function to do radiometric calibration\n",
    "    Input parameters:\n",
    "    1. source: thermal corrected data\n",
    "    2. polarization: name of intensity bands in the data\n",
    "    3. band: 'VV' or 'VH'\n",
    "    Output:\n",
    "    Return radiometric corrected data\n",
    "    '''\n",
    "    print('\\tCalibration...')\n",
    "    parameters = HashMap()\n",
    "    parameters.put('outputSigmaBand', True)\n",
    "    try:\n",
    "        parameters.put('sourceBands', polarization)\n",
    "    except:\n",
    "        print(\"different polarization!\")\n",
    "    parameters.put('selectedPolarisations', band)\n",
    "    parameters.put('outputImageScaleInDb', False)\n",
    "    output = GPF.createProduct(\"Calibration\", parameters, source)\n",
    "    return output\n",
    "\n",
    "def do_speckle_filtering(source):\n",
    "    '''\n",
    "    Function to do speckel filtering\n",
    "    Input parameters:\n",
    "    1. source: radiometric corrected data\n",
    "    Output:\n",
    "    Return speckel filtered data\n",
    "    '''\n",
    "    print('\\tSpeckle filtering...')\n",
    "    parameters = HashMap()\n",
    "    parameters.put('filter', 'Lee')\n",
    "    parameters.put('numLooksStr', '1')\n",
    "    parameters.put('filterSizeX', 5)\n",
    "    parameters.put('filterSizeY', 5)\n",
    "    # parameters.put('sigmaStr', '0.9')\n",
    "    # parameters.put('targetWindowSizeStr', '3x3')\n",
    "    # parameters.put('WindowSizeStr', '7x7')\n",
    "    output = GPF.createProduct('Speckle-Filter', parameters, source)\n",
    "    return output\n",
    "\n",
    "def do_terrain_correction(source, proj, downsample):\n",
    "    '''\n",
    "    Function to do terrain correction\n",
    "    Input parameters:\n",
    "    1. source: speckel filtered data\n",
    "    2. proj: projection for output data\n",
    "    Output:\n",
    "    Return terrain corrected data\n",
    "    '''\n",
    "    print('\\tTerrain correction...')\n",
    "    parameters = HashMap()\n",
    "    parameters.put('demName', 'SRTM 3Sec')\n",
    "    parameters.put('demResamplingMethod','BILINEAR_INTERPOLATION')\n",
    "    parameters.put('imgResamplingMethod', 'BILINEAR_INTERPOLATION')\n",
    "    while downsample == 1:\n",
    "        parameters.put('pixelSpacingInMeter', 40.0)\n",
    "        break\n",
    "    parameters.put('pixelSpacingInMeter', 10.0)\n",
    "    parameters.put('mapProjection', proj)\n",
    "    parameters.put('nodataValueAtSea', False)\n",
    "    parameters.put('saveSelectedSourceBand', True)\n",
    "#     parameters.put('saveProjectedLocalIncidenceAngle', True)\n",
    "    output = GPF.createProduct('Terrain-Correction', parameters, source)\n",
    "    return output\n",
    "\n",
    "def do_subset(source, poly_wkt):\n",
    "    '''\n",
    "    Function to create the subset of processed imagery\n",
    "    Input parameters:\n",
    "    1. source: terrain corrected data\n",
    "    2. wkt: polygon for creating subset\n",
    "    Output:\n",
    "    Return subset data\n",
    "    '''\n",
    "    print('\\tSubsetting...')\n",
    "    parameters = HashMap()\n",
    "    parameters.put('geoRegion', poly_wkt)\n",
    "    output = GPF.createProduct('Subset', parameters, source)\n",
    "    return output\n",
    "\n",
    "def covert_to_dB(source):\n",
    "    '''\n",
    "    Function to convert intensity value to db\n",
    "    Input parameters:\n",
    "    1. source: subset data/terrain corrected data\n",
    "    Output:\n",
    "    Return db converted data\n",
    "    '''\n",
    "    print('\\tCoversion to dB...')\n",
    "    parameters = HashMap()\n",
    "    output = GPF.createProduct('LinearToFromdB', parameters, source)\n",
    "    return output\n",
    "\n",
    "def s3_download(bucket_name, bucket_prefix):\n",
    "    '''\n",
    "    Function is to download the directory from s3 to local\n",
    "    Input parameters\n",
    "    1. bucket_name: Define the name of the s3 bucket where data needs to be uploaded\n",
    "    2. bucket_prefix: Define the directory path in the s3 bucket\n",
    "    '''\n",
    "    print(f'Data is downloading from s3 to local....')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    for obj in bucket.objects.filter(Prefix=bucket_prefix):\n",
    "        if not obj.key[-1] == \"/\":\n",
    "            sys_path = os.path.join(bucket_name, obj.key)\n",
    "            if not os.path.exists(os.path.dirname(sys_path)):\n",
    "                os.makedirs(os.path.dirname(sys_path))\n",
    "            dir_path = os.path.dirname(sys_path)\n",
    "            bucket.download_file(obj.key, sys_path) # save to same path\n",
    "    return dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess data based on user input\n",
    "# download raw imagery zip files from s3 to ec2 \n",
    "if download == True:\n",
    "    raw_imagery_path = s3_download(main_foldername, bucket_prefix)\n",
    "elif download == False:\n",
    "    raw_imagery_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3') # s3 client connection\n",
    "\n",
    "dict_scene = {} # define an empty dictionary to save the extent of the scene imagery based on each scene\n",
    "\n",
    "# looping through raw_imagery folder to do the preprocessing on each scene zipfile\n",
    "for file in os.listdir(raw_imagery_path):\n",
    "    if file.endswith('.zip'):\n",
    "        filename = file.split('.')[0]\n",
    "        file_path = os.path.join(raw_imagery_path, file)\n",
    "        gc.enable() # enabling the memory management\n",
    "        gc.collect() # clearing the memory for use\n",
    "        S2CacheUtils = jpy.get_type('org.esa.s2tbx.dataio.cache.S2CacheUtils')\n",
    "        sentinel_1 = ProductIO.readProduct(file_path)\n",
    "\n",
    "        ## Extract polarizations\n",
    "        band_list = list(sentinel_1.getBandNames())\n",
    "        check_bands = ['Intensity_VH', 'Intensity_VV', 'Intensity_HH', 'Intensity_HV']\n",
    "        polarization_list = []\n",
    "        pols = []\n",
    "        for check_band in check_bands:\n",
    "            if check_band in band_list:\n",
    "                pol = check_band.split('_')[1]\n",
    "                polarization_list.append(check_band)\n",
    "                pols.append(pol)\n",
    "        polarization = ','.join(polarization_list) # converting list to string\n",
    "        # Start preprocessing:\n",
    "        # appling orbit correction on the sentinel-1 image\n",
    "        applyorbit = do_apply_orbit_file(sentinel_1)\n",
    "        thermalcorrection = do_thermal_noise_removal(applyorbit)\n",
    "        scene_start = time.time()\n",
    "        ## processing for single band\n",
    "        for band in pols:\n",
    "            # appling radiometric correction on the orbit corrected data\n",
    "            calibrated = do_calibration(thermalcorrection, polarization, band)\n",
    "            filtered = do_speckle_filtering(calibrated)\n",
    "            del calibrated\n",
    "            gc.collect()\n",
    "            # appling terrain correction on the radiometric corrected data\n",
    "            tercorrected = do_terrain_correction(filtered, proj, downsample)\n",
    "            del filtered\n",
    "            gc.collect()\n",
    "            if subset == True:\n",
    "                data = do_subset(tercorrected, poly_wkt)\n",
    "            elif subset == False:\n",
    "                data = tercorrected\n",
    "            del tercorrected\n",
    "            gc.collect()\n",
    "            # converting intensity value to dB of terrain corrected data\n",
    "            convert = covert_to_dB(data)\n",
    "            del data\n",
    "            gc.collect()\n",
    "            raster_name = f'{filename}_{band}_Orb_Cal_Spk_TC_dB'\n",
    "            # creating the saving path for local and s3 each band in the scene\n",
    "            if band == 'VH':\n",
    "                output_raster_path = os.path.join(preprocessed_imagery_path_vh, raster_name)\n",
    "                drive_file_path = os.path.join(drive_path_vh, f'{raster_name}.tif')\n",
    "            elif band == 'VV':\n",
    "                output_raster_path = os.path.join(preprocessed_imagery_path_vv, raster_name)\n",
    "                drive_file_path = os.path.join(drive_path_vv, f'{raster_name}.tif')\n",
    "\n",
    "            # saving the preprocessed imagery in local instance\n",
    "            ProductIO.writeProduct(convert, output_raster_path, 'GeoTIFF')\n",
    "            del convert\n",
    "            gc.collect()\n",
    "        del applyorbit\n",
    "        gc.collect()\n",
    "        sentinel_1.dispose()\n",
    "        sentinel_1.closeIO()\n",
    "        S2CacheUtils.deleteCache()\n",
    "        \n",
    "        # reading the preprocessed imagery for getting the extent of the scene\n",
    "        local_file_path = f'{output_raster_path}.tif'\n",
    "        with rio.open(local_file_path) as src:\n",
    "            tile_bounds = src.bounds\n",
    "            polygon = Polygon([[tile_bounds.left, tile_bounds.top], [tile_bounds.left, tile_bounds.bottom], [tile_bounds.right, tile_bounds.bottom], [tile_bounds.right, tile_bounds.top]])\n",
    "            dict_scene[filename] = polygon.wkt\n",
    "        # removing the unzip folder of the scene\n",
    "        # shutil.rmtree(folder_path)\n",
    "        local_file_path = f'{output_raster_path}.tif'\n",
    "        # os.remove(local_file_path)\n",
    "        scene_end = time.time()\n",
    "        print(\"The time of execution for one scene (VH and VV) :\",(scene_end-scene_start))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
